{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AutoML for EM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdAw9af8MKfs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e717f19e-1674-42a6-f15e-9544a77bb102"
      },
      "source": [
        "# install required libraries\n",
        "import pandas as pd\n",
        "import os\n",
        "from os import path\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "import torch\n",
        "!pip install transformers==3\n",
        "\n",
        "from transformers import BertTokenizer, DistilBertTokenizer, AlbertTokenizer, RobertaTokenizer, XLNetTokenizer\n",
        "from transformers import BertModel, DistilBertModel, AlbertModel, RobertaModel, XLNetModel\n",
        "from transformers.tokenization_utils_base import BatchEncoding"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3 in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.1.94)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3) (20.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3) (1.19.4)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.8.0rc4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3) (2.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64E3nblVMsNn"
      },
      "source": [
        "# Data Collector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQBOWCSVoxyS"
      },
      "source": [
        "class DataCollector(object):\n",
        "  def __init__(self, data_dir):\n",
        "\n",
        "    if not isinstance(data_dir, str):\n",
        "      raise TypeError(\"Wrong data directory type.\")\n",
        "\n",
        "    !mkdir -p $data_dir\n",
        "    \n",
        "    self.data_dir = data_dir\n",
        "    self.available_use_cases = [\"Structured/DBLP-GoogleScholar\", \n",
        "                                \"Structured/DBLP-ACM\", \n",
        "                                \"Structured/Amazon-Google\",\n",
        "                                \"Structured/Walmart-Amazon\",\n",
        "                                \"Structured/Beer\", \"Structured/iTunes-Amazon\",\n",
        "                                \"Structured/Fodors-Zagats\", \"Textual/Abt-Buy\",\n",
        "                                \"Textual/Company\", \"Dirty/iTunes-Amazon\",\n",
        "                                \"Dirty/DBLP-ACM\", \"Dirty/DBLP-GoogleScholar\",\n",
        "                                \"Dirty/Walmart-Amazon\"]\n",
        "\n",
        "  def _get_complete_dataset(self, dataset):\n",
        "    \"\"\"\n",
        "    Expand the dataset with the records from source A and source B.\n",
        "    \"\"\"\n",
        "    \n",
        "    dataset_path = os.path.join(self.data_dir, dataset)\n",
        "    ds = pd.read_csv(dataset_path)\n",
        "\n",
        "    tableA_path = os.path.join(self.data_dir, \"tableA.csv\")\n",
        "    ds_a = pd.read_csv(tableA_path)\n",
        "\n",
        "    tableB_path = os.path.join(self.data_dir, \"tableB.csv\")\n",
        "    ds_b = pd.read_csv(tableB_path)\n",
        "\n",
        "    assert 'ltable_id' in ds\n",
        "    assert 'rtable_id' in ds\n",
        "    assert 'id' in ds_b\n",
        "    assert 'id' in ds_a\n",
        "\n",
        "    ds_a = ds_a.add_prefix('left_')\n",
        "    ds_b = ds_b.add_prefix('right_')\n",
        "\n",
        "    ds = pd.merge(ds, ds_a, how='inner', left_on='ltable_id', right_on='left_id', suffixes=(False, False))\n",
        "    ds = pd.merge(ds, ds_b, how='inner', left_on='rtable_id', right_on='right_id', suffixes=(False, False))\n",
        "\n",
        "    ds.drop([\"ltable_id\", \"rtable_id\"], axis=1, inplace=True)\n",
        "\n",
        "    return ds\n",
        "\n",
        "  def _save_complete_dataset(self, dataset):\n",
        "    \"\"\"\n",
        "    Expand the integrated dataset.\n",
        "    \"\"\"\n",
        "    \n",
        "    ds = self._get_complete_dataset(dataset)\n",
        "\n",
        "    out_file_name = os.path.join(self.data_dir, dataset)\n",
        "    ds.to_csv(out_file_name, index=False)\n",
        "\n",
        "  def _download_data(self, use_case):\n",
        "    \"\"\"\n",
        "    Download the datasets associated to the provided DeepMatcher use case.\n",
        "    \"\"\"\n",
        "  \n",
        "    base_url = \"http://pages.cs.wisc.edu/~anhai/data1/deepmatcher_data/{}/exp_data\".format(use_case)\n",
        "    tableA = \"{}/tableA.csv\".format(base_url)\n",
        "    tableB = \"{}/tableB.csv\".format(base_url)\n",
        "    train = \"{}/train.csv\".format(base_url)\n",
        "    test = \"{}/test.csv\".format(base_url)\n",
        "    valid = \"{}/valid.csv\".format(base_url)\n",
        "\n",
        "    !wget -P $self.data_dir $tableA\n",
        "    !wget -P $self.data_dir $tableB\n",
        "    !wget -P $self.data_dir $train\n",
        "    !wget -P $self.data_dir $test\n",
        "    !wget -P $self.data_dir $valid\n",
        "\n",
        "    # extend datasets\n",
        "    self._save_complete_dataset(\"train.csv\")\n",
        "    self._save_complete_dataset(\"test.csv\")\n",
        "    self._save_complete_dataset(\"valid.csv\")\n",
        "\n",
        "  def get_data(self, use_case):\n",
        "\n",
        "    if not isinstance(use_case, str):\n",
        "      raise TypeError(\"Wrong use case type.\")\n",
        "\n",
        "    if use_case not in self.available_use_cases:\n",
        "      raise ValueError(\"Wrong use case name.\")\n",
        "\n",
        "    # check data existence\n",
        "    if not (os.path.exists(os.path.join(self.data_dir, \"tableA.csv\")) and \\\n",
        "       os.path.exists(os.path.join(self.data_dir, \"tableB.csv\")) and \\\n",
        "       os.path.exists(os.path.join(self.data_dir, \"train.csv\")) and \\\n",
        "       os.path.exists(os.path.join(self.data_dir, \"valid.csv\")) and \\\n",
        "       os.path.exists(os.path.join(self.data_dir, \"test.csv\"))):\n",
        "      \n",
        "      print(\"Starting downloading the data...\")\n",
        "      for file in [\"tableA.csv\", \"tableB.csv\", \"train.csv\", \"valid.csv\", \"test.csv\"]:\n",
        "        f = os.path.join(self.data_dir, file)\n",
        "        !rm $f\n",
        "\n",
        "      self._download_data(use_case)\n",
        "    \n",
        "    else:\n",
        "      print(\"Data already downloaded.\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9PIyRZMSAjK"
      },
      "source": [
        "# use_case = \"Textual/Abt-Buy\"\n",
        "# data_dir = \"data/{}/original/\".format(use_case)\n",
        "\n",
        "# data_collector = DataCollector(data_dir)\n",
        "# data_collector.get_data(use_case)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57HUxXFNs7nA"
      },
      "source": [
        "# Data Containers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtJ_YtCEs-o-"
      },
      "source": [
        "class DataContainer(object):\n",
        "  \n",
        "  def __init__(self, data_dir, dataset_id, dataset_file, columns, label_col,\n",
        "               sample_size=None, left_prefix='left_', right_prefix='right_',\n",
        "               seed=24):\n",
        "\n",
        "    # check parameter types\n",
        "    if not isinstance(data_dir, str):\n",
        "      raise TypeError(\"Wrong data dir type.\")\n",
        "\n",
        "    if not isinstance(dataset_id, str):\n",
        "      raise TypeError(\"Wrong dataset id type.\")\n",
        "\n",
        "    if not isinstance(dataset_file, str):\n",
        "      raise TypeError(\"Wrong dataset file type.\")\n",
        "\n",
        "    if not isinstance(columns, list):\n",
        "      raise TypeError(\"Wrong columns type.\")\n",
        "\n",
        "    if not isinstance(label_col, str):\n",
        "      raise TypeError(\"Wrong label column type.\")\n",
        "\n",
        "    if sample_size is not None:\n",
        "      if not isinstance(sample_size, int):\n",
        "        raise TypeError(\"Wrong sample size type.\")\n",
        "\n",
        "    if not isinstance(left_prefix, str):\n",
        "      raise TypeError(\"Wrong left prefix type.\")\n",
        "\n",
        "    if not isinstance(right_prefix, str):\n",
        "      raise TypeError(\"Wrong right prefix type.\")\n",
        "\n",
        "    # check parameter values\n",
        "    if not os.path.exists(data_dir):\n",
        "      raise ValueError(\"Data directory not found.\")\n",
        "\n",
        "    dataset_path = os.path.join(data_dir, dataset_file)\n",
        "    if not os.path.exists(dataset_path):\n",
        "      raise ValueError(\"Dataset file not found.\")\n",
        "    \n",
        "    data = pd.read_csv(dataset_path)\n",
        "\n",
        "    if len(columns) == 0:\n",
        "      return ValueError(\"No columns provided.\")\n",
        "      \n",
        "    for col in columns:\n",
        "      for prefix in [left_prefix, right_prefix]:\n",
        "        c = \"{}{}\".format(prefix, col)\n",
        "        if c not in data.columns.values:\n",
        "          raise ValueError(\"No column {} found.\".format(c)) \n",
        "\n",
        "    if label_col not in data.columns.values:\n",
        "      raise ValueError(\"No column {} found.\".format(label_col))  \n",
        "\n",
        "    if sample_size is not None and dataset_id == 'train':\n",
        "\n",
        "      print(\"Getting {} data sample with {} elements.\".format(dataset_id, sample_size))\n",
        "      match = data[data[label_col] == 1]\n",
        "      non_match = data[data[label_col] == 0]\n",
        "      match_ratio = len(match) / len(data)\n",
        "\n",
        "      if sample_size > len(data):\n",
        "        raise ValueError(\"Wrong sample size value.\")\n",
        "\n",
        "      match_sample_size = int(round(match_ratio * sample_size))\n",
        "      match_sample = match.sample(n=match_sample_size, random_state=seed)\n",
        "      non_match_sample = non_match.sample(n=sample_size - len(match_sample),\n",
        "                                          random_state=seed)\n",
        "      data_sample = pd.concat([match_sample, non_match_sample])\n",
        "      data = data_sample.sample(frac=1, random_state=seed)\n",
        "      print(\"Data sample match ratio: {}\".format((len(data[data[label_col]==1]) / len(data))*100))\n",
        "\n",
        "    self.nrows = data.shape[0]\n",
        "    self.ncols = len(columns)\n",
        "    self.match_ratio = (len(data[data[label_col]==1]) / len(data))*100\n",
        "\n",
        "    self.dataset_id = dataset_id\n",
        "    self.label_col = label_col\n",
        "    self.all_left_cols = [\"{}{}\".format(left_prefix, c) for c in columns]\n",
        "    self.all_right_cols = [\"{}{}\".format(right_prefix, c) for c in columns]\n",
        "    self.all_cols = self.all_left_cols + self.all_right_cols# + [label_col]\n",
        "    complete_data = data.copy()\n",
        "    self.complete_data = complete_data[self.all_cols]\n",
        "\n",
        "    data.drop([\"{}id\".format(left_prefix), \"{}id\".format(right_prefix)], axis=1, inplace=True)\n",
        "\n",
        "    self.labels = data[label_col]\n",
        "    data.drop([label_col], axis=1, inplace=True)\n",
        "    self.data = data   \n",
        "    self.columns = columns.copy()\n",
        "    self.left_cols = self.all_left_cols\n",
        "    self.right_cols = self.all_right_cols\n",
        "    self.left_prefix = left_prefix\n",
        "    self.right_prefix = right_prefix\n",
        "\n",
        "    self.tokenized_data = None\n",
        "    self.encoded_data = None\n",
        "    self.embeddings = None\n",
        "\n",
        "  def get_complete_data(self):\n",
        "    return self.complete_data\n",
        "\n",
        "  def get_data(self):    \n",
        "    return self.data\n",
        "\n",
        "  def get_stats(self):\n",
        "    return {\"nrows\": self.nrows, \"ncols\": self.ncols, \"match_ratio\": self.match_ratio}\n",
        "\n",
        "  def get_labels(self):\n",
        "    return self.labels\n",
        "\n",
        "  def get_label_col(self):\n",
        "    return self.label_col\n",
        "\n",
        "  def get_dataset_id(self):\n",
        "    return self.dataset_id\n",
        "\n",
        "  def get_columns(self):\n",
        "    return self.columns\n",
        "\n",
        "  def get_left_columns(self):\n",
        "    return self.left_cols\n",
        "\n",
        "  def get_right_columns(self):\n",
        "    return self.right_cols\n",
        "\n",
        "  def set_tokenized_data(self, tokenized_data):\n",
        "    self.tokenized_data = tokenized_data\n",
        "\n",
        "  def get_tokenized_data(self):\n",
        "    return self.tokenized_data\n",
        "\n",
        "  def set_encoded_data(self, encoded_data):\n",
        "    self.encoded_data = encoded_data\n",
        "\n",
        "  def get_encoded_data(self):\n",
        "    return self.encoded_data\n",
        "\n",
        "  def set_embeddings(self, embeddings):\n",
        "    self.embeddings = embeddings\n",
        "\n",
        "  def get_embeddings(self):\n",
        "    return self.embeddings\n",
        "\n",
        "  def check_em_adapter_application(self):\n",
        "    return self.tokenized_data is not None and self.encoded_data is not None and self.embeddings is not None"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfvsgSOC_Aal"
      },
      "source": [
        "class DataCollectionContainer(object):\n",
        "  \n",
        "  def __init__(self, data_dir, collection_id, train_file, valid_file, test_file,\n",
        "               columns, label_col, sample_size=None, left_prefix='left_',\n",
        "               right_prefix='right_'):    \n",
        "\n",
        "    if not isinstance(collection_id, str):\n",
        "      raise TypeError(\"Wrong collection id type.\")          \n",
        "\n",
        "    self.collection_id = collection_id\n",
        "    self.train = DataContainer(data_dir, \"train\", train_file, columns, label_col,\n",
        "                               sample_size, left_prefix, right_prefix)\n",
        "    self.valid = DataContainer(data_dir, \"valid\", valid_file, columns, label_col,\n",
        "                               sample_size, left_prefix, right_prefix)\n",
        "    self.test = DataContainer(data_dir, \"test\", test_file, columns, label_col,\n",
        "                              sample_size, left_prefix, right_prefix)\n",
        "    self.files = {\"train\": self.train, \"valid\": self.valid, \"test\": self.test}\n",
        "    self.left_cols = [\"{}{}\".format(left_prefix, c) for c in columns]\n",
        "    self.right_cols = [\"{}{}\".format(right_prefix, c) for c in columns]\n",
        "    self.all_cols = self.left_cols + self.right_cols + [label_col]  \n",
        "    self.columns = columns\n",
        "    self.label_col = label_col\n",
        "\n",
        "  def get_data(self, data_id):\n",
        "    \n",
        "    if not isinstance(data_id, str):\n",
        "      raise TypeError(\"Wrong data_id type.\")\n",
        "\n",
        "    if data_id not in list(self.files):\n",
        "      raise ValueError(\"Wrong data_id value.\")\n",
        "\n",
        "    return self.files[data_id]\n",
        "\n",
        "  def get_stats(self):\n",
        "    \n",
        "    stats_data = {}\n",
        "    for data_id in self.files:\n",
        "      stats = self.files[data_id].get_stats()\n",
        "      for stats_item in stats:\n",
        "        stats_data[\"{}_{}\".format(data_id, stats_item)] = stats[stats_item]\n",
        "\n",
        "    return stats_data\n",
        "\n",
        "  def get_columns(self):\n",
        "    return self.columns\n",
        "\n",
        "  def get_label_col(self):\n",
        "    return self.label_col\n",
        "\n",
        "  def get_columns_and_label(self):\n",
        "    return self.all_cols\n",
        "\n",
        "  def get_left_columns(self):\n",
        "    return self.left_cols\n",
        "\n",
        "  def get_right_columns(self):\n",
        "    return self.right_cols"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD09DaR40kez"
      },
      "source": [
        "# cols = list(pd.read_csv(os.path.join(data_dir, \"tableA.csv\")).columns.values)[1:]\n",
        "# label_col = \"label\"\n",
        "# sample_size = 500\n",
        "# data_container_col = DataCollectionContainer(data_dir, use_case, \"train.csv\", \"valid.csv\", \"test.csv\", cols, label_col, sample_size)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhiUYrCkHXp3"
      },
      "source": [
        "# cols = list(pd.read_csv(os.path.join(data_dir, \"tableA.csv\")).columns.values)[1:]\n",
        "# label_col = \"label\"\n",
        "# sample_size = 200\n",
        "# data_container = DataContainer(data_dir, \"test\", \"test.csv\", cols, label_col, sample_size)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wVS89bONgjT"
      },
      "source": [
        "# EM Adapter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkPnBcyhNpl8"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV7nvfceNogc"
      },
      "source": [
        "class EM_Tokenizer(object):\n",
        "  \"\"\"\n",
        "  Component that tokenizes EM datasets.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, name, max_length=512):\n",
        "    \"\"\"\n",
        "    This method initializes the EM_Tokenizer by selecting the Huggingface's\n",
        "    tokenizer associated to the provided tokenizer name.\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(name, str):\n",
        "      raise TypeError(\"Wrong tokenizer name type.\")\n",
        "\n",
        "    available_tokenizers = [\"bert\", \"distilbert\", \"albert\", \"roberta\", \"xlnet\"]\n",
        "    if name not in available_tokenizers:\n",
        "      raise ValueError(\"Tokenizer not found.\")\n",
        "    \n",
        "    if name == 'bert':\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    elif name == 'distilbert':\n",
        "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    elif name == 'albert':\n",
        "        tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
        "\n",
        "    elif name == 'roberta':\n",
        "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "    elif name == 'xlnet':\n",
        "        tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')        \n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "    self.additional_special_tokens = None\n",
        "\n",
        "  def set_max_length(self, max_length):\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def _tokenize_pair_sent(self, entity1, entity2):\n",
        "\n",
        "      sent1 = ' '.join([str(val) for val in entity1.to_list()])# if val != unk_token])\n",
        "      sent2 = ' '.join([str(val) for val in entity2.to_list()])# if val != unk_token])\n",
        "      \n",
        "      return [sent1], [sent2]\n",
        "\n",
        "  def _tokenize_pair_attr(self, entity1, entity2):\n",
        "    \n",
        "    sent1 = []\n",
        "    sent2 = []\n",
        "\n",
        "    cols1 = list(entity1.index)\n",
        "    cols2 = list(entity2.index)\n",
        "    for pair_col in zip(cols1, cols2):\n",
        "      attr_sent1 = str(entity1[pair_col[0]])\n",
        "      attr_sent2 = str(entity2[pair_col[1]])\n",
        "      sent1.append(attr_sent1)\n",
        "      sent2.append(attr_sent2)\n",
        "    \n",
        "    return sent1, sent2\n",
        "\n",
        "  def _tokenize_pair_incremental_attr(self, entity1, entity2):\n",
        "\n",
        "    sentences1 = []\n",
        "    sentences2 = []\n",
        "\n",
        "    cols1 = list(entity1.index)\n",
        "    cols2 = list(entity2.index)\n",
        "    sent1 = \"\"\n",
        "    sent2 = \"\"\n",
        "    for pair_col in zip(cols1, cols2):\n",
        "      attr_sent1 = str(entity1[pair_col[0]])\n",
        "      attr_sent2 = str(entity2[pair_col[1]])\n",
        "\n",
        "      if sent1 == \"\":\n",
        "        sent1 = attr_sent1\n",
        "      else:\n",
        "        sent1 += \" {}\".format(attr_sent1)\n",
        "\n",
        "      if sent2 == \"\":\n",
        "        sent2 = attr_sent2\n",
        "      else:\n",
        "        sent2 += \" {}\".format(attr_sent2)\n",
        "\n",
        "      sentences1.append(sent1)\n",
        "      sentences2.append(sent2)\n",
        "\n",
        "    return sentences1, sentences2\n",
        "\n",
        "  def _tokenize_pair_sent_attr(self, entity1, entity2):\n",
        "\n",
        "    special_tokens = []\n",
        "\n",
        "    sent1 = \"\"\n",
        "    for col in entity1.index:\n",
        "      col_special_token = \"[{}]\".format(col)\n",
        "      sent1 += \"{} {} {} \".format(col_special_token, entity1[col], col_special_token)\n",
        "\n",
        "      if self.additional_special_tokens is None:\n",
        "        special_tokens.append(col_special_token)\n",
        "    sent1 = sent1[:-1]\n",
        "\n",
        "    if len(special_tokens) > 0:\n",
        "      print(\"Adding new special tokens: {}\".format(special_tokens))      \n",
        "      self.additional_special_tokens = special_tokens\n",
        "      self.tokenizer.add_tokens(self.additional_special_tokens)\n",
        "      print(\"Now the tokens are: {} (+{})\".format(len(self.tokenizer), len(special_tokens)))\n",
        "    \n",
        "    sent2 = \"\"\n",
        "    for col in entity2.index:\n",
        "      col_special_token = \"[{}]\".format(col)\n",
        "      sent2 += \"{} {} {} \".format(col_special_token, entity2[col], col_special_token)\n",
        "    sent2 = sent2[:-1]\n",
        "\n",
        "    return [sent1], [sent2]\n",
        "\n",
        "  def tokenize(self, entity1, entity2, method):\n",
        "      \"\"\"\n",
        "      This method tokenizes a pair of entity mentions.\n",
        "      \"\"\"\n",
        "\n",
        "      unk_token = self.tokenizer.unk_token\n",
        "      entity1 = entity1.fillna(unk_token)\n",
        "      entity2 = entity2.fillna(unk_token)\n",
        "\n",
        "      sentences1 = []\n",
        "      sentences2 = []\n",
        "      \n",
        "      if method == 'pair-sent':\n",
        "        \n",
        "        sent1, sent2 = self._tokenize_pair_sent(entity1, entity2)\n",
        "        sentences1 += sent1\n",
        "        sentences2 += sent2\n",
        "\n",
        "      elif method == 'pair-attr':\n",
        "\n",
        "        sent1, sent2 = self._tokenize_pair_attr(entity1, entity2)\n",
        "        sentences1 += sent1\n",
        "        sentences2 += sent2\n",
        "\n",
        "      elif method == 'pair-incremental-attr':\n",
        "\n",
        "        sent1, sent2 = self._tokenize_pair_incremental_attr(entity1, entity2)\n",
        "        sentences1 += sent1\n",
        "        sentences2 += sent2\n",
        "\n",
        "      elif method == 'pair-sent-attr':\n",
        "        \n",
        "        sent1, sent2 = self._tokenize_pair_sent_attr(entity1, entity2)\n",
        "        sentences1 += sent1\n",
        "        sentences2 += sent2\n",
        "\n",
        "      elif method == \"pair-sent-incremental-attr\":\n",
        "\n",
        "        sent1, sent2 = self._tokenize_pair_attr(entity1, entity2)\n",
        "        sentences1 += sent1\n",
        "        sentences2 += sent2\n",
        "\n",
        "        sent1, sent2 = self._tokenize_pair_sent_attr(entity1, entity2)\n",
        "        sentences1 += sent1\n",
        "        sentences2 += sent2        \n",
        "\n",
        "      token_data_items = self.tokenizer(sentences1, sentences2, padding='max_length',\n",
        "                                        truncation=True, return_tensors=\"pt\",\n",
        "                                        max_length = self.max_length,\n",
        "                                        add_special_tokens = True, \n",
        "                                        pad_to_max_length = True, \n",
        "                                        return_attention_mask = True)\n",
        "\n",
        "      # n = len(token_data_items[\"input_ids\"])\n",
        "      # for i in range(n):\n",
        "      #   print(self.tokenizer.decode(token_data_items[\"input_ids\"][i]))\n",
        "\n",
        "      return token_data_items\n",
        "\n",
        "  def tokenize_dataset(self, data_container, method, force=True):\n",
        "      \"\"\"\n",
        "      This method tokenizes all rows of a EM dataset.\n",
        "      \"\"\"\n",
        "\n",
        "      if not isinstance(data_container, DataContainer):\n",
        "        raise TypeError(\"Wrong data container type.\")\n",
        "\n",
        "      if not isinstance(method, str):\n",
        "        raise TypeError(\"Wrong method type.\")    \n",
        "\n",
        "      available_methods = [\"pair-sent\", \"pair-attr\", \"pair-sent-attr\",\n",
        "                           \"pair-incremental-attr\", \"pair-sent-incremental-attr\"]\n",
        "      if method not in available_methods:\n",
        "        raise ValueError(\"Tokenization method not found.\")    \n",
        "\n",
        "      if not force:\n",
        "        if data_container.get_tokenized_data():\n",
        "          return data_container.get_tokenized_data()\n",
        "\n",
        "      if method == 'pair-attr':\n",
        "        self.max_length = 128\n",
        "      else:\n",
        "        if not data_container.get_dataset_id().startswith(\"Textual\"):\n",
        "          self.max_length = 256\n",
        "        else:\n",
        "          self.max_length = 512\n",
        "\n",
        "      df = data_container.get_data()  \n",
        "      left_columns = data_container.get_left_columns()\n",
        "      right_columns = data_container.get_right_columns()\n",
        "      columns = data_container.get_columns()\n",
        "\n",
        "      tokenized_data = []\n",
        "      for row_id, row in tqdm(df.iterrows()):\n",
        "        left_row = row[left_columns]\n",
        "        left_row.index = columns\n",
        "        right_row = row[right_columns]\n",
        "        right_row.index = columns\n",
        "        tokenized_row = self.tokenize(left_row, right_row, method)\n",
        "        tokenized_data.append(tokenized_row)\n",
        "\n",
        "      return tokenized_data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oWULUPzu7Rw"
      },
      "source": [
        "# # tokenize_method = 'pair-sent'\n",
        "# #tokenize_method = 'pair-attr'\n",
        "# # tokenize_method = 'pair-incremental-attr'\n",
        "# tokenize_method = 'pair-sent-attr'\n",
        "# #tokenize_method = 'pair-sent-incremental-attr'\n",
        "\n",
        "# em_tokenizer = EM_Tokenizer('bert')\n",
        "# tokenized_data = em_tokenizer.tokenize_dataset(data_container, tokenize_method)\n",
        "# data_container.set_tokenized_data(tokenized_data)\n",
        "# tokenized_data"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjmE4h0LN1cr"
      },
      "source": [
        "## Embedder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaWYHK9PN5DY"
      },
      "source": [
        "class Embedder(object):\n",
        "  \"\"\"\n",
        "  Component that encodes tokenized data into pre-trained embeddings.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, name, em_tokenizer=None):\n",
        "    \"\"\"\n",
        "    This method initializes the Embedder by selecting the Huggingface's\n",
        "    pre-trained models.\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(name, str):\n",
        "      raise TypeError(\"Wrong model name type.\")\n",
        "\n",
        "    available_models = [\"bert\", \"distilbert\", \"albert\", \"roberta\", \"xlnet\"]\n",
        "    if name not in available_models:\n",
        "      raise ValueError(\"Model not found.\")\n",
        "\n",
        "    self.name = name\n",
        "\n",
        "    if name == 'bert':\n",
        "      self.embedder = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
        "    \n",
        "    elif name == 'distilbert':\n",
        "      self.embedder = DistilBertModel.from_pretrained('distilbert-base-uncased', output_hidden_states=True)\n",
        "    \n",
        "    elif name == 'albert':\n",
        "      self.embedder = AlbertModel.from_pretrained('albert-base-v2', output_hidden_states=True)\n",
        "\n",
        "    elif name == 'roberta':\n",
        "      self.embedder = RobertaModel.from_pretrained('roberta-base', output_hidden_states=True)\n",
        "\n",
        "    elif name == 'xlnet':\n",
        "      self.embedder = XLNetModel.from_pretrained('xlnet-base-cased', output_hidden_states=True)\n",
        "\n",
        "    # resize the model in order to insert in the embedding matrix also new vectors\n",
        "    # for the new added special tokens\n",
        "    # the vectors of these special tokens will be randomly initialized\n",
        "    if em_tokenizer:\n",
        "      print(\"Embedding matrix resize: the new dimension is {}.\".format(len(em_tokenizer.tokenizer)))\n",
        "      self.embedder.resize_token_embeddings(len(em_tokenizer.tokenizer))\n",
        "\n",
        "    # Set the device to GPU (SELECT THE GPU RUNTIME)\n",
        "    self.device = torch.device(0)\n",
        "\n",
        "    self.embedder.eval()\n",
        "    self.embedder = self.embedder.to(self.device)\n",
        "    \n",
        "  def get_encoded_data(self, data):\n",
        "    \n",
        "    if not isinstance(data, BatchEncoding):\n",
        "      raise TypeError(\"Wrong data type.\")\n",
        "\n",
        "    required_params = [\"input_ids\", \"attention_mask\"]\n",
        "    for param in required_params:\n",
        "      if param not in data:\n",
        "        raise ValueError(\"Wrong data value.\")\n",
        "\n",
        "    optional_params = [\"token_type_ids\"]\n",
        "    if self.name in [\"bert\", \"albert\", \"xlnet\"]:\n",
        "      for param in optional_params:\n",
        "        if param not in data:\n",
        "          raise ValueError(\"Wrong data value.\")\n",
        "\n",
        "    input_ids = [data[\"input_ids\"]]\n",
        "    input_ids = torch.cat(input_ids, dim=0)  \n",
        "    attention_mask = [data[\"attention_mask\"]]\n",
        "    attention_mask = torch.cat(attention_mask, dim=0)  \n",
        "    token_type_ids = None\n",
        "    if self.name in [\"bert\", \"albert\", \"xlnet\"]:\n",
        "      token_type_ids = [data[\"token_type_ids\"]]\n",
        "      token_type_ids = torch.cat(token_type_ids, dim=0)  \n",
        "        \n",
        "    input_ids = input_ids.to(self.device)    \n",
        "    attention_mask = attention_mask.to(self.device)\n",
        "    if token_type_ids is not None:\n",
        "      token_type_ids = token_type_ids.to(self.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      if token_type_ids is not None:\n",
        "        outputs = self.embedder(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "      \n",
        "      else:\n",
        "        outputs = self.embedder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    if self.name == 'distilbert' or self.name == 'xlnet':\n",
        "      hidden_states = outputs[1]\n",
        "    else:\n",
        "      hidden_states = outputs[2]\n",
        "\n",
        "    if self.name == \"bert\":\n",
        "\n",
        "      # # get last four layers\n",
        "      # last_four_layers = [hidden_states[i] for i in (-1, -2, -3, -4)]\n",
        "      # # cast layers to a tuple and concatenate over the last dimension\n",
        "      # cat_hidden_states = torch.cat(tuple(last_four_layers), dim=-1)\n",
        "      # # take the mean of the concatenated vector over the token dimension\n",
        "      # encoded_data = torch.mean(cat_hidden_states, dim=1).squeeze()\n",
        "\n",
        "      # get last hidden layer\n",
        "      encoded_data = torch.mean(hidden_states[-1], dim=1).squeeze()\n",
        "\n",
        "    else:\n",
        "\n",
        "      # get last layer and take the mean of the concatenated vector over the token dimension\n",
        "      encoded_data = torch.mean(hidden_states[-1], dim=1).squeeze()\n",
        "\n",
        "    return encoded_data\n",
        "\n",
        "  def get_encoded_batch_data(self, batch_data):    \n",
        "    \n",
        "    if not isinstance(batch_data, list):\n",
        "      raise TypeError(\"Wrong batch data type.\")\n",
        "\n",
        "    encoded_batch_data = []\n",
        "    for data in tqdm(batch_data):\n",
        "      encoded_data = self.get_encoded_data(data)        \n",
        "      encoded_batch_data.append(encoded_data)\n",
        "\n",
        "    return encoded_batch_data"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fN9oeqBgzhL"
      },
      "source": [
        "# emb = Embedder(\"bert\", em_tokenizer=em_tokenizer)\n",
        "# encoded_data = emb.get_encoded_batch_data(tokenized_data)\n",
        "# data_container.set_encoded_data(encoded_data)\n",
        "# encoded_data"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAeaYdKfsExj"
      },
      "source": [
        "## Combiner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfVgXK8asEUf"
      },
      "source": [
        "class Combiner(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def combine(self, embeddings, method):\n",
        "    \n",
        "    if not isinstance(method, str):\n",
        "      raise TypeError(\"Wrong method type.\") \n",
        "\n",
        "    methods = ['avg']\n",
        "    if method not in methods:\n",
        "      raise ValueError(\"Wrong method value.\")\n",
        "\n",
        "    if not torch.is_tensor(embeddings):\n",
        "      raise ValueError(\"Wrong embeddings data value.\")\n",
        "    \n",
        "    out_embedding = None\n",
        "    nrows = list(embeddings.size())[0]\n",
        "    if nrows > 1:    \n",
        "\n",
        "        if method == 'avg':\n",
        "          summary_vec = torch.mean(embeddings, dim=0)\n",
        "\n",
        "        out_embedding = summary_vec.cpu().detach().numpy()\n",
        "\n",
        "    else:\n",
        "      # no combiner is need, but a new data format is generated\n",
        "\n",
        "      out_embedding = embeddings.cpu().detach().numpy()\n",
        "\n",
        "    return out_embedding\n",
        "\n",
        "  def combine_batch_data(self, list_embeddings, method):\n",
        "\n",
        "    if not isinstance(list_embeddings, list):\n",
        "      raise TypeError(\"Wrong embeddings type.\")       \n",
        "    \n",
        "    if len(list_embeddings) == 0:\n",
        "      raise ValueError(\"Empty embedding list provided.\")\n",
        "\n",
        "    nrows = len(list_embeddings)\n",
        "    emb_size = list(list_embeddings[0].size())\n",
        "    ncols = emb_size[0]\n",
        "    if len(emb_size) > 1:\n",
        "      ncols = emb_size[1]\n",
        "    out_embeddings = np.empty((nrows, ncols))\n",
        "\n",
        "    for idx, embeddings in enumerate(list_embeddings):\n",
        "\n",
        "      out_embedding = self.combine(embeddings, method)\n",
        "      out_embeddings[idx, :] = out_embedding\n",
        "\n",
        "    return out_embeddings"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUZdnXVdRqAo"
      },
      "source": [
        "# combiner_method = 'avg'\n",
        "# combiner = Combiner()\n",
        "# embeddings = combiner.combine_batch_data(encoded_data, combiner_method)\n",
        "# data_container.set_embeddings(embeddings)\n",
        "# embeddings"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIGb2pDrjume"
      },
      "source": [
        "## EM adapter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDBwlWzij2Zo"
      },
      "source": [
        "class EM_Adapter(object):\n",
        "  def __init__(self, data_container, model):\n",
        "\n",
        "    self.data_container = data_container\n",
        "    self.model = model   \n",
        "    self.em_tokenizer = None\n",
        "    self.embedder = None\n",
        "    self.combiner = None    \n",
        "    self.times = {}\n",
        "\n",
        "  def _tokenize_data(self, tokenize_method):\n",
        "\n",
        "    print(\"\\nApplying {} tokenization...\".format(tokenize_method))\n",
        "\n",
        "    ts = time.time()\n",
        "\n",
        "    self.em_tokenizer = EM_Tokenizer(self.model)\n",
        "    tokenized_data = self.em_tokenizer.tokenize_dataset(self.data_container, tokenize_method)\n",
        "\n",
        "    ts2 = time.time()\n",
        "\n",
        "    elapsed_time = ts2 - ts\n",
        "    self.times[\"tokenization\"] = elapsed_time\n",
        "\n",
        "    print(\"Adding tokenized data to data container.\")\n",
        "\n",
        "    self.data_container.set_tokenized_data(tokenized_data)\n",
        "\n",
        "    print(\"Tokenization {} applied successfully.\\n\".format(tokenize_method))\n",
        "\n",
        "    return tokenized_data\n",
        "\n",
        "  def _encode_data(self, tokenized_data):\n",
        "\n",
        "    print(\"\\nEncoding data with {}...\".format(self.model))\n",
        "\n",
        "    ts = time.time()\n",
        "\n",
        "    self.embedder = Embedder(self.model, em_tokenizer=self.em_tokenizer)\n",
        "    encoded_data = self.embedder.get_encoded_batch_data(tokenized_data)\n",
        "\n",
        "    ts2 = time.time()\n",
        "\n",
        "    elapsed_time = ts2 - ts\n",
        "    self.times[\"embedding\"] = elapsed_time\n",
        "\n",
        "    print(\"Adding encoded data to data container.\")\n",
        "\n",
        "    self.data_container.set_encoded_data(encoded_data)\n",
        "\n",
        "    print(\"Data encoded successfully.\\n\".format(self.model))\n",
        "\n",
        "    return encoded_data\n",
        "\n",
        "  def _create_embeddings(self, encoded_data, combiner_method):\n",
        "    \n",
        "    print(\"\\nCreating {} embeddings...\".format(self.model))\n",
        "\n",
        "    ts = time.time()\n",
        "\n",
        "    self.combiner = Combiner()\n",
        "    embeddings = self.combiner.combine_batch_data(encoded_data, combiner_method)\n",
        "\n",
        "    ts2 = time.time()\n",
        "\n",
        "    elapsed_time = ts2 - ts\n",
        "    self.times[\"combining\"] = elapsed_time\n",
        "\n",
        "    print(\"Embedding matrix: {}\".format(embeddings.shape))\n",
        "\n",
        "    print(\"Adding embeddings to data container.\")\n",
        "\n",
        "    self.data_container.set_embeddings(embeddings)\n",
        "\n",
        "    print(\"{} embeddings created successfully.\\n\".format(self.model))\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "  def transform_data(self, tokenize_method, combiner_method):\n",
        "\n",
        "    tokenized_data = self._tokenize_data(tokenize_method)\n",
        "    encoded_data = self._encode_data(tokenized_data)\n",
        "    embeddings = self._create_embeddings(encoded_data, combiner_method)\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "  def get_times(self):\n",
        "    return self.times"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbmSmfJ_fbHO"
      },
      "source": [
        "# em_adapter = EM_Adapter(data_container, \"bert\")\n",
        "\n",
        "# # tokenize_method = 'pair-sent'\n",
        "# #tokenize_method = 'pair-attr'\n",
        "# # tokenize_method = 'pair-incremental-attr'\n",
        "# #tokenize_method = 'pair-sent-attr'\n",
        "# tokenize_method = 'pair-sent-incremental-attr'\n",
        "\n",
        "# combiner_method = 'avg'\n",
        "\n",
        "# adapted_data = em_adapter.transform_data(tokenize_method, combiner_method)\n",
        "# adapted_data"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKk8az5KDf0O"
      },
      "source": [
        "class EMAdapterCollection(object):\n",
        "  \n",
        "  def __init__(self, container_collection, model):\n",
        "\n",
        "    if not isinstance(container_collection, DataCollectionContainer):\n",
        "      raise TypeError(\"Wrong container collection data type.\")\n",
        "\n",
        "    if not isinstance(model, str):\n",
        "      raise TypeError(\"Wrong model data type.\")\n",
        "\n",
        "    print(\"\\nInitializing EM adapter based on {} model...\".format(model))\n",
        "\n",
        "    train_container = container_collection.get_data(\"train\")\n",
        "    test_container = container_collection.get_data(\"test\")\n",
        "\n",
        "    self.train_em_adapter = EM_Adapter(train_container, model)\n",
        "    self.test_em_adapter = EM_Adapter(test_container, model)\n",
        "\n",
        "    self.times = {}\n",
        "\n",
        "    print(\"EM adapter initialized successfully.\\n\")\n",
        "\n",
        "  def transform_data(self, tokenize_method, combiner_method):\n",
        "\n",
        "    print(\"\\nAdapting train data with {} tokenization and {} combiner...\".format(tokenize_method, combiner_method))\n",
        "    train_embeddings = self.train_em_adapter.transform_data(tokenize_method, combiner_method)\n",
        "    self.times[\"train\"] = self.train_em_adapter.get_times()\n",
        "    print(\"Train data adapted successfully.\\n\")\n",
        "\n",
        "    print(\"\\nAdapting test data with {} tokenization and {} combiner...\".format(tokenize_method, combiner_method))\n",
        "    test_embeddings = self.test_em_adapter.transform_data(tokenize_method, combiner_method)\n",
        "    self.times[\"test\"] = self.test_em_adapter.get_times()\n",
        "    print(\"Test data adapted successfully.\\n\")\n",
        "\n",
        "    return train_embeddings, test_embeddings\n",
        "\n",
        "  def get_times(self):\n",
        "    return self.times"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJKdk_BlFfXC"
      },
      "source": [
        "# em_adapter_col = EMAdapterCollection(data_container_col, \"bert\")\n",
        "\n",
        "# #tokenize_method = 'pair-sent'\n",
        "# #tokenize_method = 'pair-attr'\n",
        "# tokenize_method = 'pair-incremental-attr'\n",
        "# #tokenize_method = 'pair-sent-attr'\n",
        "# #tokenize_method = 'pair-sent-incremental-attr'\n",
        "\n",
        "# combiner_method = 'avg'\n",
        "\n",
        "# train_adapted_data, test_adapted_data = em_adapter_col.transform_data(tokenize_method, combiner_method)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpUVAgeZbTff"
      },
      "source": [
        "# AutoML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZIhpSpO8HWF"
      },
      "source": [
        "import time\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "class AutoML(object):\n",
        "  def __init__(self, name, container_collection, train_time=None, seed=24):\n",
        "\n",
        "    if not isinstance(name, str):\n",
        "      raise TypeError(\"Wrong name data type.\")\n",
        "\n",
        "    if not isinstance(container_collection, DataCollectionContainer):\n",
        "      raise TypeError(\"Wrong container collection type.\")\n",
        "\n",
        "    automl_names = [\"autosklearn\", \"h2o\", \"autogluon\"]\n",
        "    if name not in automl_names:\n",
        "      raise ValueError(\"Wrong name value.\")\n",
        "    \n",
        "    self.name = name\n",
        "    self.label_col = container_collection.get_label_col()\n",
        "    self.train = container_collection.get_data(\"train\")\n",
        "    self.test = container_collection.get_data(\"test\")    \n",
        "    self.train.check_em_adapter_application()\n",
        "    self.test.check_em_adapter_application()\n",
        "    self.library = None\n",
        "\n",
        "    self.x_train = self.train.get_embeddings()    \n",
        "    self.y_train = self.train.get_labels()\n",
        "    self.x_train_tabular = pd.DataFrame(self.x_train)\n",
        "    self.y_train_tabular = pd.DataFrame(self.y_train)\n",
        "    self.train_tabular = pd.concat([self.x_train_tabular, self.y_train_tabular], axis=1)\n",
        "    \n",
        "    self.x_test = self.test.get_embeddings()    \n",
        "    self.y_test = self.test.get_labels() \n",
        "    assert not (self.y_test == 0).all()   \n",
        "    assert not (self.y_test == 1).all()   \n",
        "    self.x_test_tabular = pd.DataFrame(self.x_test)\n",
        "    self.y_test_tabular = pd.DataFrame(self.y_test)\n",
        "    self.test_tabular = pd.concat([self.x_test_tabular, self.y_test_tabular], axis=1)\n",
        "\n",
        "    self.train_time = train_time\n",
        "    self.seed = seed\n",
        "    self.model = None\n",
        "\n",
        "    if name == 'autosklearn':\n",
        "\n",
        "      if train_time: \n",
        "        self.model = AutoSklearn2Classifier(n_jobs=-1,\n",
        "                      metric=autosklearn.metrics.f1,\n",
        "                      time_left_for_this_task=self.train_time, seed=seed)\n",
        "      else:\n",
        "        self.model = AutoSklearn2Classifier(n_jobs=-1,\n",
        "                      metric=autosklearn.metrics.f1, seed=seed)\n",
        "                    \n",
        "    elif name == 'h2o':\n",
        "\n",
        "      self.library = h2o\n",
        "            \n",
        "      self.train_tabular = h2o.H2OFrame(self.train_tabular)\n",
        "      self.train_tabular[self.label_col] = self.train_tabular[self.label_col].asfactor()\n",
        "\n",
        "      self.test_tabular = h2o.H2OFrame(self.test_tabular)\n",
        "      self.test_tabular[self.label_col] = self.test_tabular[self.label_col].asfactor()\n",
        "\n",
        "      if train_time:\n",
        "        self.model = H2OAutoML(seed=seed, stopping_metric=\"AUC\", sort_metric=\"AUC\",\n",
        "                              max_runtime_secs=self.train_time)\n",
        "      else:\n",
        "        self.model = H2OAutoML(seed=seed, stopping_metric=\"AUC\", sort_metric=\"AUC\")              \n",
        "\n",
        "  def fit(self):\n",
        "\n",
        "    print(\"Strating training process...\")\n",
        "    \n",
        "    ts = time.time()\n",
        "    \n",
        "    if self.name == 'autosklearn':\n",
        "      \n",
        "      try:\n",
        "        self.model.fit(self.x_train, self.y_train)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "        self.model = None\n",
        "\n",
        "    elif self.name == 'h2o':\n",
        "\n",
        "      self.model.train(x=self.train_tabular.columns[:-1], y=self.label_col,\n",
        "                       training_frame=self.train_tabular)\n",
        "\n",
        "    elif self.name == 'autogluon':\n",
        "\n",
        "      from autogluon.tabular import TabularPrediction as task\n",
        "\n",
        "      hyperparameters = {  # hyperparameters of each model type\n",
        "                   'NN': {'seed_value': self.seed}, \n",
        "                   'GBM': {'seed': self.seed},\n",
        "                   'CAT': {'random_seed': self.seed},\n",
        "                   'RF': {'random_state': self.seed},\n",
        "                   'XT': {'random_state': self.seed},\n",
        "                   'KNN': {'n_jobs': -1},\n",
        "                   'LR': {'random_state': self.seed}\n",
        "                  }\n",
        "      \n",
        "      if self.train_time:\n",
        "\n",
        "        try:\n",
        "          self.model = task.fit(train_data=self.train_tabular, label=self.label_col,\n",
        "                              eval_metric=\"f1\", presets='best_quality',\n",
        "                              auto_stack=True, time_limits=self.train_time,\n",
        "                              random_seed=self.seed, hyperparameters=hyperparameters)\n",
        "        except ValueError as e:\n",
        "\n",
        "          if str(e) == \"AutoGluon did not successfully train any models\":\n",
        "            self.model = None\n",
        "\n",
        "      else:\n",
        "\n",
        "        try:\n",
        "          self.model = task.fit(train_data=self.train_tabular, label=self.label_col,\n",
        "                              eval_metric=\"f1\", presets='best_quality',\n",
        "                              auto_stack=True, random_seed=self.seed,\n",
        "                              hyperparameters=hyperparameters)\n",
        "        except ValueError as e:\n",
        "\n",
        "          if str(e) == \"AutoGluon did not successfully train any models\":\n",
        "            self.model = None\n",
        "      \n",
        "    ts2 = time.time()\n",
        "\n",
        "    elapsed_time = ts2 - ts\n",
        "    \n",
        "    print(\"Training process completed in {}s.\".format(elapsed_time))\n",
        "\n",
        "    return elapsed_time\n",
        "\n",
        "  def predict(self):\n",
        "\n",
        "    print(\"Strating prediction process...\")\n",
        "    \n",
        "    ts = time.time()\n",
        "    \n",
        "    x_test = self.x_test_tabular\n",
        "\n",
        "    if self.name == 'h2o':      \n",
        "      x_test = self.test_tabular\n",
        "    \n",
        "    elif self.name == 'autosklearn':\n",
        "      x_test = x_test.values\n",
        "      \n",
        "    y_pred = None\n",
        "    if self.model is not None:\n",
        "      y_pred = self.model.predict(x_test)\n",
        "\n",
        "    if self.name == 'h2o':\n",
        "      if y_pred is not None:\n",
        "        y_pred = self.library.as_list(y_pred).iloc[:, 0]\n",
        "    \n",
        "    ts2 = time.time()\n",
        "    elapsed_time = ts2 - ts\n",
        "    \n",
        "    print(\"Prediction process completed in {}s.\".format(elapsed_time))\n",
        "\n",
        "    return y_pred, elapsed_time\n",
        "\n",
        "  def get_f1_score(self, y_pred):\n",
        "\n",
        "    if y_pred is None:\n",
        "      return None\n",
        "\n",
        "    return f1_score(self.y_test, y_pred)\n",
        "    "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR2BP27r-Mfj"
      },
      "source": [
        "# # model = 'autosklearn'\n",
        "# # model = 'h2o'\n",
        "# model = 'autogluon'\n",
        "# automl = AutoML(model, data_container_col)\n",
        "# automl.fit()\n",
        "# y_pred, _ = automl.predict()\n",
        "# f1 = automl.get_f1_score(y_pred)\n",
        "# f1"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQfyOSd4M2Ch"
      },
      "source": [
        "# Executor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZK0PdrOu6Qt"
      },
      "source": [
        "class ExecutorConfiguration(object):\n",
        "  def __init__(self, use_case, sample_size, embedding_model, tokenizer_name,\n",
        "               combiner_method, automl_model, automl_train_time):\n",
        "      \n",
        "    data = {}\n",
        "    data[\"use_case\"] = use_case\n",
        "    data[\"data_dir\"] = \"data/{}/original/\".format(use_case)\n",
        "    data[\"label_col\"] = \"label\"\n",
        "    data[\"sample_size\"] = sample_size\n",
        "    data[\"embedding_model\"] = embedding_model\n",
        "    data[\"tokenizer_name\"] = tokenizer_name\n",
        "    data[\"combiner_method\"] = combiner_method\n",
        "    data[\"automl_model\"] = automl_model\n",
        "    data[\"automl_train_time\"] = automl_train_time\n",
        "    \n",
        "    self.data = data\n",
        "\n",
        "  def get_data(self):\n",
        "    return self.data\n",
        "\n",
        "  def get_param(self, name):\n",
        "    return self.data[name]\n",
        "\n",
        "  def __str__(self):\n",
        "\n",
        "    use_case  = self.data[\"use_case\"]\n",
        "    sample_size  = self.data[\"sample_size\"]\n",
        "    embedding  = self.data[\"embedding_model\"]\n",
        "    tokenizer  = self.data[\"tokenizer_name\"]\n",
        "    combiner  = self.data[\"combiner_method\"]\n",
        "    automl  = self.data[\"automl_model\"]\n",
        "    automl_train_time  = self.data[\"automl_train_time\"]\n",
        "\n",
        "    return \"USE CASE: {}, SAMPLE SIZE: {}, EMBEDDING: {}, TOKENIZER: {}, COMBINER: {}, AUTOML: {}, AUTOML TRAIN TIME: {}\".format(use_case, sample_size, embedding, tokenizer, combiner, automl, automl_train_time)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HYHpsIamNvW"
      },
      "source": [
        "class Executor(object):\n",
        "  def __init__(self, conf):\n",
        "\n",
        "    if not isinstance(conf, ExecutorConfiguration):\n",
        "      raise TypeError(\"Wrong configuration data type.\")\n",
        "\n",
        "    self.conf = conf\n",
        "    \n",
        "    data_dir = self.conf.get_param(\"data_dir\")\n",
        "    use_case = self.conf.get_param(\"use_case\")    \n",
        "    label_col = self.conf.get_param(\"label_col\")\n",
        "    sample_size = self.conf.get_param(\"sample_size\")\n",
        "    self.embedding_model = self.conf.get_param(\"embedding_model\")\n",
        "    self.tokenizer_name = self.conf.get_param(\"tokenizer_name\")\n",
        "    self.combiner_method = self.conf.get_param(\"combiner_method\")\n",
        "    self.automl_model = self.conf.get_param(\"automl_model\")\n",
        "    self.automl_train_time = self.conf.get_param(\"automl_train_time\")\n",
        "    \n",
        "    data_collector = DataCollector(data_dir)\n",
        "    data_collector.get_data(use_case)\n",
        "    \n",
        "    cols = list(pd.read_csv(os.path.join(data_dir, \"tableA.csv\")).columns.values)[1:]\n",
        "    self.data = DataCollectionContainer(data_dir, use_case, \"train.csv\",\n",
        "                                        \"valid.csv\", \"test.csv\", cols,\n",
        "                                        label_col, sample_size)\n",
        "    self.em_adapter_times = {}\n",
        "    \n",
        "    print(str(self.conf))\n",
        "  \n",
        "  def get_conf(self):\n",
        "    return str(self.conf)\n",
        "\n",
        "  def adapt_data_for_em(self):\n",
        "\n",
        "    em_adapter_col = EMAdapterCollection(self.data, self.embedding_model)\n",
        "\n",
        "    _, _ = em_adapter_col.transform_data(self.tokenizer_name,\n",
        "                                         self.combiner_method)\n",
        "    flat_em_adapter_times = {}\n",
        "    em_adapter_times = em_adapter_col.get_times()\n",
        "    for data_id in em_adapter_times:\n",
        "      for task in em_adapter_times[data_id]:\n",
        "        flat_em_adapter_times[\"{}_{}\".format(data_id, task)] = em_adapter_times[data_id][task]\n",
        "    self.em_adapter_times = flat_em_adapter_times\n",
        "  \n",
        "  def run(self):\n",
        "\n",
        "    automl = AutoML(self.automl_model, self.data, self.automl_train_time)\n",
        "    train_time = automl.fit()\n",
        "    y_pred, pred_time = automl.predict()\n",
        "    f1 = automl.get_f1_score(y_pred)\n",
        "\n",
        "    results = self.conf.get_data()\n",
        "    results[\"train_time\"] = train_time\n",
        "    results[\"test_time\"] = pred_time\n",
        "    results[\"score\"] = f1\n",
        "    results.update(self.em_adapter_times)\n",
        "    results.update(self.data.get_stats())    \n",
        "    \n",
        "    return pd.DataFrame([results])\n",
        "    "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wND8FCFyKCgr"
      },
      "source": [
        "# Use case\n",
        "# use_case = \"Structured/DBLP-GoogleScholar\"\n",
        "# use_case = \"Structured/DBLP-ACM\"\n",
        "# use_case = \"Structured/Amazon-Google\"\n",
        "# use_case = \"Structured/Walmart-Amazon\"\n",
        "use_case = \"Structured/Beer\"\n",
        "# use_case = \"Structured/iTunes-Amazon\"\n",
        "# use_case = \"Structured/Fodors-Zagats\"\n",
        "# use_case = \"Textual/Abt-Buy\"\n",
        "# use_case = \"Textual/Company\"\n",
        "# use_case = \"Dirty/iTunes-Amazon\"\n",
        "# use_case = \"Dirty/DBLP-ACM\"\n",
        "# use_case = \"Dirty/DBLP-GoogleScholar\"\n",
        "# use_case = \"Dirty/Walmart-Amazon\"\n",
        "\n",
        "# Sample size\n",
        "sample_size = None\n",
        "# sample_size = 200\n",
        "\n",
        "# EM adapter\n",
        "\n",
        "## embedding model\n",
        "embedding_model = \"bert\"\n",
        "# embedding_model = \"distilbert\"\n",
        "# embedding_model = \"albert\"\n",
        "# embedding_model = \"roberta\"\n",
        "# embedding_model = \"xlnet\"\n",
        "\n",
        "## tokenizer\n",
        "# tokenizer_name = 'pair-sent'\n",
        "# tokenizer_name = 'pair-attr'\n",
        "# tokenizer_name = 'pair-incremental-attr'\n",
        "tokenizer_name = 'pair-sent-attr'\n",
        "# tokenizer_name = 'pair-sent-incremental-attr'\n",
        "\n",
        "## combiner\n",
        "combiner_method = 'avg'\n",
        "\n",
        "# AutoML\n",
        "# automl_model = 'autosklearn'\n",
        "automl_model = \"h2o\"\n",
        "# automl_model = \"autogluon\"\n",
        "\n",
        "## train time\n",
        "automl_train_time = None\n",
        "# automl_train_time = 600"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vlw8Y3T_ZgkT",
        "outputId": "6719345e-4cce-4af7-c0fa-28b15997ed2e"
      },
      "source": [
        "target_automl = automl_model\n",
        "if target_automl == 'autosklearn':\n",
        "\n",
        "  !sudo apt-get install build-essential swig\n",
        "  !curl https://raw.githubusercontent.com/automl/auto-sklearn/master/requirements.txt | xargs -n 1 -L 1 pip install\n",
        "  !pip install auto-sklearn\n",
        "\n",
        "  for _ in range(3):\n",
        "      try:\n",
        "          import autosklearn.classification\n",
        "          break\n",
        "      except:\n",
        "          pass\n",
        "  else:\n",
        "      raise ImportError(\"failed to import from autosklearn\")\n",
        "\n",
        "  from autosklearn.experimental.askl2 import AutoSklearn2Classifier\n",
        "                \n",
        "elif target_automl == 'h2o':\n",
        "\n",
        "  !pip install h2o\n",
        "\n",
        "  import h2o\n",
        "  from h2o.automl import H2OAutoML\n",
        "\n",
        "  h2o.init()\n",
        "\n",
        "elif target_automl == 'autogluon':\n",
        "\n",
        "  !pip install dask==2020.12\n",
        "  !pip install --upgrade setuptools\n",
        "  !pip install --upgrade \"mxnet<2.0.0\"\n",
        "  !pip install --pre autogluon"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dask==2020.12 in /usr/local/lib/python3.6/dist-packages (2020.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from dask==2020.12) (3.13)\n",
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (51.1.1)\n",
            "Requirement already up-to-date: mxnet<2.0.0 in /usr/local/lib/python3.6/dist-packages (1.7.0.post1)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet<2.0.0) (1.19.4)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet<2.0.0) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet<2.0.0) (0.8.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet<2.0.0) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet<2.0.0) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet<2.0.0) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet<2.0.0) (2020.12.5)\n",
            "Requirement already satisfied: autogluon in /usr/local/lib/python3.6/dist-packages (0.0.16b20210107)\n",
            "Requirement already satisfied: autogluon.core==0.0.16b20210107 in /usr/local/lib/python3.6/dist-packages (from autogluon) (0.0.16b20210107)\n",
            "Requirement already satisfied: autogluon.extra==0.0.16b20210107 in /usr/local/lib/python3.6/dist-packages (from autogluon) (0.0.16b20210107)\n",
            "Requirement already satisfied: autogluon.vision==0.0.16b20210107 in /usr/local/lib/python3.6/dist-packages (from autogluon) (0.0.16b20210107)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from autogluon) (3.6.4)\n",
            "Requirement already satisfied: autogluon.text==0.0.16b20210107 in /usr/local/lib/python3.6/dist-packages (from autogluon) (0.0.16b20210107)\n",
            "Requirement already satisfied: autogluon.mxnet==0.0.16b20210107 in /usr/local/lib/python3.6/dist-packages (from autogluon) (0.0.16b20210107)\n",
            "Requirement already satisfied: autogluon.tabular==0.0.16b20210107 in /usr/local/lib/python3.6/dist-packages (from autogluon) (0.0.16b20210107)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from autogluon.core==0.0.16b20210107->autogluon) (1.16.50)\n",
            "Requirement already satisfied: pandas<2.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from autogluon.core==0.0.16b20210107->autogluon) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn<0.24,>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from autogluon.core==0.0.16b20210107->autogluon) (0.22.2.post1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from autogluon.core==0.0.16b20210107->autogluon) (2.23.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from autogluon.core==0.0.16b20210107->autogluon) (3.2.2)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.6/dist-packages (from autogluon.core==0.0.16b20210107->autogluon) (0.8.1)\n",
            "Requirement already satisfied: distributed>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from autogluon.core==0.0.16b20210107->autogluon) (2020.12.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from autogluon.core==0.0.16b20210107->autogluon) (0.8.4)\n",
            "Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.6/dist-packages (from autogluon.core==0.0.16b20210107->autogluon) (1.3)\n",
            "Requirement already satisfied: scipy<1.5.0,>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from autogluon.core==0.0.16b20210107->autogluon) (1.4.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from autogluon.core==0.0.16b20210107->autogluon) (0.29.21)\n",
            "Requirement already satisfied: ConfigSpace<=0.4.16 in /usr/local/lib/python3.6/dist-packages (from autogluon.core==0.0.16b20210107->autogluon) (0.4.16)\n",
            "Requirement already satisfied: dask>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from autogluon.core==0.0.16b20210107->autogluon) (2020.12.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from autogluon.core==0.0.16b20210107->autogluon) (1.19.4)\n",
            "Requirement already satisfied: tornado>=5.0.1 in /usr/local/lib/python3.6/dist-packages (from autogluon.core==0.0.16b20210107->autogluon) (5.1.1)\n",
            "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.6/dist-packages (from autogluon.core==0.0.16b20210107->autogluon) (2.7.2)\n",
            "Requirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from autogluon.core==0.0.16b20210107->autogluon) (4.41.1)\n",
            "Requirement already satisfied: dill==0.3.3 in /usr/local/lib/python3.6/dist-packages (from autogluon.core==0.0.16b20210107->autogluon) (0.3.3)\n",
            "Requirement already satisfied: gluoncv<1.0,>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from autogluon.extra==0.0.16b20210107->autogluon) (0.10.0b20210107)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.6/dist-packages (from autogluon.extra==0.0.16b20210107->autogluon) (2.1.1)\n",
            "Requirement already satisfied: openml in /usr/local/lib/python3.6/dist-packages (from autogluon.extra==0.0.16b20210107->autogluon) (0.11.0)\n",
            "Requirement already satisfied: d8<1.0,>=0.0.2 in /usr/local/lib/python3.6/dist-packages (from autogluon.vision==0.0.16b20210107->autogluon) (0.0.2)\n",
            "Requirement already satisfied: Pillow<=6.2.1 in /usr/local/lib/python3.6/dist-packages (from autogluon.vision==0.0.16b20210107->autogluon) (6.2.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->autogluon) (51.1.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->autogluon) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->autogluon) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->autogluon) (8.6.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->autogluon) (1.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->autogluon) (20.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->autogluon) (1.15.0)\n",
            "Requirement already satisfied: autogluon-contrib-nlp in /usr/local/lib/python3.6/dist-packages (from autogluon.text==0.0.16b20210107->autogluon) (0.0.1b20201009)\n",
            "Requirement already satisfied: pyarrow<=1.0.0 in /usr/local/lib/python3.6/dist-packages (from autogluon.text==0.0.16b20210107->autogluon) (0.14.1)\n",
            "Requirement already satisfied: psutil<=5.7.0,>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from autogluon.tabular==0.0.16b20210107->autogluon) (5.4.8)\n",
            "Requirement already satisfied: catboost<0.25,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from autogluon.tabular==0.0.16b20210107->autogluon) (0.24.4)\n",
            "Requirement already satisfied: fastai<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from autogluon.tabular==0.0.16b20210107->autogluon) (1.0.61)\n",
            "Requirement already satisfied: torch<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from autogluon.tabular==0.0.16b20210107->autogluon) (1.7.0+cu101)\n",
            "Requirement already satisfied: xgboost<1.3,>=1.2 in /usr/local/lib/python3.6/dist-packages (from autogluon.tabular==0.0.16b20210107->autogluon) (1.2.1)\n",
            "Requirement already satisfied: networkx<3.0,>=2.3 in /usr/local/lib/python3.6/dist-packages (from autogluon.tabular==0.0.16b20210107->autogluon) (2.5)\n",
            "Requirement already satisfied: lightgbm<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from autogluon.tabular==0.0.16b20210107->autogluon) (3.1.1)\n",
            "Requirement already satisfied: botocore<1.20.0,>=1.19.50 in /usr/local/lib/python3.6/dist-packages (from boto3->autogluon.core==0.0.16b20210107->autogluon) (1.19.50)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->autogluon.core==0.0.16b20210107->autogluon) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->autogluon.core==0.0.16b20210107->autogluon) (0.3.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas<2.0,>=1.0.0->autogluon.core==0.0.16b20210107->autogluon) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas<2.0,>=1.0.0->autogluon.core==0.0.16b20210107->autogluon) (2.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn<0.24,>=0.22.0->autogluon.core==0.0.16b20210107->autogluon) (1.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->autogluon.core==0.0.16b20210107->autogluon) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->autogluon.core==0.0.16b20210107->autogluon) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->autogluon.core==0.0.16b20210107->autogluon) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->autogluon.core==0.0.16b20210107->autogluon) (3.0.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->autogluon.core==0.0.16b20210107->autogluon) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->autogluon.core==0.0.16b20210107->autogluon) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->autogluon.core==0.0.16b20210107->autogluon) (0.10.0)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize->autogluon.core==0.0.16b20210107->autogluon) (20.4.0)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.6.0->autogluon.core==0.0.16b20210107->autogluon) (7.1.2)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.6.0->autogluon.core==0.0.16b20210107->autogluon) (2.3.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.6.0->autogluon.core==0.0.16b20210107->autogluon) (1.7.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.6.0->autogluon.core==0.0.16b20210107->autogluon) (0.11.1)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.6.0->autogluon.core==0.0.16b20210107->autogluon) (1.6.0)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.6.0->autogluon.core==0.0.16b20210107->autogluon) (1.0.1)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.6.0->autogluon.core==0.0.16b20210107->autogluon) (2.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from distributed>=2.6.0->autogluon.core==0.0.16b20210107->autogluon) (3.13)\n",
            "Requirement already satisfied: contextvars; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from distributed>=2.6.0->autogluon.core==0.0.16b20210107->autogluon) (2.4)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from autograd>=1.3->autogluon.core==0.0.16b20210107->autogluon) (0.16.0)\n",
            "Requirement already satisfied: pynacl>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from paramiko>=2.4->autogluon.core==0.0.16b20210107->autogluon) (1.4.0)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.6/dist-packages (from paramiko>=2.4->autogluon.core==0.0.16b20210107->autogluon) (3.3.1)\n",
            "Requirement already satisfied: bcrypt>=3.1.3 in /usr/local/lib/python3.6/dist-packages (from paramiko>=2.4->autogluon.core==0.0.16b20210107->autogluon) (3.2.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from gluoncv<1.0,>=0.9.1->autogluon.extra==0.0.16b20210107->autogluon) (2.0.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gluoncv<1.0,>=0.9.1->autogluon.extra==0.0.16b20210107->autogluon) (4.1.2.30)\n",
            "Requirement already satisfied: autocfg in /usr/local/lib/python3.6/dist-packages (from gluoncv<1.0,>=0.9.1->autogluon.extra==0.0.16b20210107->autogluon) (0.0.6)\n",
            "Requirement already satisfied: yacs in /usr/local/lib/python3.6/dist-packages (from gluoncv<1.0,>=0.9.1->autogluon.extra==0.0.16b20210107->autogluon) (0.1.8)\n",
            "Requirement already satisfied: decord in /usr/local/lib/python3.6/dist-packages (from gluoncv<1.0,>=0.9.1->autogluon.extra==0.0.16b20210107->autogluon) (0.4.2)\n",
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.6/dist-packages (from gluoncv<1.0,>=0.9.1->autogluon.extra==0.0.16b20210107->autogluon) (2.1)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.6/dist-packages (from bokeh->autogluon.extra==0.0.16b20210107->autogluon) (20.8)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh->autogluon.extra==0.0.16b20210107->autogluon) (2.11.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from bokeh->autogluon.extra==0.0.16b20210107->autogluon) (3.7.4.3)\n",
            "Requirement already satisfied: liac-arff>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from openml->autogluon.extra==0.0.16b20210107->autogluon) (2.5.0)\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.6/dist-packages (from openml->autogluon.extra==0.0.16b20210107->autogluon) (0.12.0)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (from d8<1.0,>=0.0.2->autogluon.vision==0.0.16b20210107->autogluon) (1.5.10)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from autogluon-contrib-nlp->autogluon.text==0.0.16b20210107->autogluon) (2019.12.20)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.6/dist-packages (from autogluon-contrib-nlp->autogluon.text==0.0.16b20210107->autogluon) (1.4.14)\n",
            "Requirement already satisfied: flake8 in /usr/local/lib/python3.6/dist-packages (from autogluon-contrib-nlp->autogluon.text==0.0.16b20210107->autogluon) (3.8.4)\n",
            "Requirement already satisfied: sacremoses>=0.0.38 in /usr/local/lib/python3.6/dist-packages (from autogluon-contrib-nlp->autogluon.text==0.0.16b20210107->autogluon) (0.0.43)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from autogluon-contrib-nlp->autogluon.text==0.0.16b20210107->autogluon) (3.12.4)\n",
            "Requirement already satisfied: tokenizers<0.9.0,>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from autogluon-contrib-nlp->autogluon.text==0.0.16b20210107->autogluon) (0.8.0rc4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from autogluon-contrib-nlp->autogluon.text==0.0.16b20210107->autogluon) (0.1.94)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost<0.25,>=0.23.0->autogluon.tabular==0.0.16b20210107->autogluon) (4.4.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fastai<2.0,>=1.0->autogluon.tabular==0.0.16b20210107->autogluon) (0.8)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.6/dist-packages (from fastai<2.0,>=1.0->autogluon.tabular==0.0.16b20210107->autogluon) (7.352.0)\n",
            "Requirement already satisfied: spacy>=2.0.18; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from fastai<2.0,>=1.0->autogluon.tabular==0.0.16b20210107->autogluon) (2.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from fastai<2.0,>=1.0->autogluon.tabular==0.0.16b20210107->autogluon) (4.6.3)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.6/dist-packages (from fastai<2.0,>=1.0->autogluon.tabular==0.0.16b20210107->autogluon) (1.3.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from fastai<2.0,>=1.0->autogluon.tabular==0.0.16b20210107->autogluon) (2.7.1)\n",
            "Requirement already satisfied: fastprogress>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from fastai<2.0,>=1.0->autogluon.tabular==0.0.16b20210107->autogluon) (1.0.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai<2.0,>=1.0->autogluon.tabular==0.0.16b20210107->autogluon) (0.8.1+cu101)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx<3.0,>=2.3->autogluon.tabular==0.0.16b20210107->autogluon) (4.4.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.6/dist-packages (from lightgbm<4.0,>=3.0->autogluon.tabular==0.0.16b20210107->autogluon) (0.36.2)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.6/dist-packages (from zict>=0.1.3->distributed>=2.6.0->autogluon.core==0.0.16b20210107->autogluon) (1.0.1)\n",
            "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars; python_version < \"3.7\"->distributed>=2.6.0->autogluon.core==0.0.16b20210107->autogluon) (0.14)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from pynacl>=1.0.1->paramiko>=2.4->autogluon.core==0.0.16b20210107->autogluon) (1.14.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh->autogluon.extra==0.0.16b20210107->autogluon) (1.1.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle->d8<1.0,>=0.0.2->autogluon.vision==0.0.16b20210107->autogluon) (4.0.1)\n",
            "Requirement already satisfied: pycodestyle<2.7.0,>=2.6.0a1 in /usr/local/lib/python3.6/dist-packages (from flake8->autogluon-contrib-nlp->autogluon.text==0.0.16b20210107->autogluon) (2.6.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from flake8->autogluon-contrib-nlp->autogluon.text==0.0.16b20210107->autogluon) (3.3.0)\n",
            "Requirement already satisfied: pyflakes<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from flake8->autogluon-contrib-nlp->autogluon.text==0.0.16b20210107->autogluon) (2.2.0)\n",
            "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from flake8->autogluon-contrib-nlp->autogluon.text==0.0.16b20210107->autogluon) (0.6.1)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost<0.25,>=0.23.0->autogluon.tabular==0.0.16b20210107->autogluon) (1.3.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai<2.0,>=1.0->autogluon.tabular==0.0.16b20210107->autogluon) (0.8.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai<2.0,>=1.0->autogluon.tabular==0.0.16b20210107->autogluon) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai<2.0,>=1.0->autogluon.tabular==0.0.16b20210107->autogluon) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai<2.0,>=1.0->autogluon.tabular==0.0.16b20210107->autogluon) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai<2.0,>=1.0->autogluon.tabular==0.0.16b20210107->autogluon) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai<2.0,>=1.0->autogluon.tabular==0.0.16b20210107->autogluon) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai<2.0,>=1.0->autogluon.tabular==0.0.16b20210107->autogluon) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai<2.0,>=1.0->autogluon.tabular==0.0.16b20210107->autogluon) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai<2.0,>=1.0->autogluon.tabular==0.0.16b20210107->autogluon) (7.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.4.1->pynacl>=1.0.1->paramiko>=2.4->autogluon.core==0.0.16b20210107->autogluon) (2.20)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle->d8<1.0,>=0.0.2->autogluon.vision==0.0.16b20210107->autogluon) (1.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->flake8->autogluon-contrib-nlp->autogluon.text==0.0.16b20210107->autogluon) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "griUuLQqaJW_"
      },
      "source": [
        "# If a package error is raised, try to restart the runtime"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QdfF8kzWZb-F",
        "outputId": "a0dd6534-3571-42cc-a33e-6495a492d8a2"
      },
      "source": [
        "conf = ExecutorConfiguration(use_case, sample_size, embedding_model, \n",
        "                             tokenizer_name, combiner_method, automl_model,\n",
        "                             automl_train_time)\n",
        "\n",
        "executor = Executor(conf)\n",
        "executor.adapt_data_for_em()\n",
        "results = executor.run()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32it [00:00, 315.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Data already downloaded.\n",
            "USE CASE: Structured/Beer, SAMPLE SIZE: None, EMBEDDING: bert, TOKENIZER: pair-sent-attr, COMBINER: avg, AUTOML: autogluon, AUTOML TRAIN TIME: 600\n",
            "\n",
            "Initializing EM adapter based on bert model...\n",
            "EM adapter initialized successfully.\n",
            "\n",
            "\n",
            "Adapting train data with pair-sent-attr tokenization and avg combiner...\n",
            "\n",
            "Applying pair-sent-attr tokenization...\n",
            "Adding new special tokens: ['[Beer_Name]', '[Brew_Factory_Name]', '[Style]', '[ABV]']\n",
            "Now the tokens are: 30526 (+4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "268it [00:00, 314.11it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Adding tokenized data to data container.\n",
            "Tokenization pair-sent-attr applied successfully.\n",
            "\n",
            "\n",
            "Encoding data with bert...\n",
            "Embedding matrix resize: the new dimension is 30526.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 268/268 [00:04<00:00, 60.26it/s]\n",
            "33it [00:00, 328.54it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Adding encoded data to data container.\n",
            "Data encoded successfully.\n",
            "\n",
            "\n",
            "Creating bert embeddings...\n",
            "Embedding matrix: (268, 768)\n",
            "Adding embeddings to data container.\n",
            "bert embeddings created successfully.\n",
            "\n",
            "Train data adapted successfully.\n",
            "\n",
            "\n",
            "Adapting test data with pair-sent-attr tokenization and avg combiner...\n",
            "\n",
            "Applying pair-sent-attr tokenization...\n",
            "Adding new special tokens: ['[Beer_Name]', '[Brew_Factory_Name]', '[Style]', '[ABV]']\n",
            "Now the tokens are: 30526 (+4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "91it [00:00, 319.37it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Adding tokenized data to data container.\n",
            "Tokenization pair-sent-attr applied successfully.\n",
            "\n",
            "\n",
            "Encoding data with bert...\n",
            "Embedding matrix resize: the new dimension is 30526.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 91/91 [00:01<00:00, 57.59it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Adding encoded data to data container.\n",
            "Data encoded successfully.\n",
            "\n",
            "\n",
            "Creating bert embeddings...\n",
            "Embedding matrix: (91, 768)\n",
            "Adding embeddings to data container.\n",
            "bert embeddings created successfully.\n",
            "\n",
            "Test data adapted successfully.\n",
            "\n",
            "Strating training process...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "No output_directory specified. Models will be saved in: AutogluonModels/ag-20210107_133844/\n",
            "Beginning AutoGluon training ... Time limit = 600s\n",
            "AutoGluon will save models to AutogluonModels/ag-20210107_133844/\n",
            "AutoGluon Version:  0.0.16b20210107\n",
            "Train Data Rows:    268\n",
            "Train Data Columns: 768\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
            "\t2 unique label values:  [0, 1]\n",
            "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    9743.17 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.65 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 768 | ['0', '1', '2', '3', '4', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 768 | ['0', '1', '2', '3', '4', ...]\n",
            "\t0.7s = Fit runtime\n",
            "\t768 features in original data used to generate 768 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.65 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.76s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'f1'\n",
            "\tTo change this, specify the eval_metric argument of fit()\n",
            "AutoGluon will early stop models using evaluation metric: 'f1'\n",
            "Fitting model: RandomForest_BAG_L0 ... Training model for up to 599.24s of the 599.23s of remaining time.\n",
            "\t0.0\t = Validation f1 score\n",
            "\t5.35s\t = Training runtime\n",
            "\t0.51s\t = Validation runtime\n",
            "Fitting model: ExtraTrees_BAG_L0 ... Training model for up to 593.29s of the 593.28s of remaining time.\n",
            "\t0.0\t = Validation f1 score\n",
            "\t3.66s\t = Training runtime\n",
            "\t0.51s\t = Validation runtime\n",
            "Fitting model: KNeighbors_BAG_L0 ... Training model for up to 589.02s of the 589.01s of remaining time.\n",
            "\t0.0\t = Validation f1 score\n",
            "\t0.16s\t = Training runtime\n",
            "\t0.51s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L0 ... Training model for up to 588.31s of the 588.31s of remaining time.\n",
            "\t0.0\t = Validation f1 score\n",
            "\t6.59s\t = Training runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L0 ... Training model for up to 581.66s of the 581.66s of remaining time.\n",
            "\t0.0\t = Validation f1 score\n",
            "\t172.08s\t = Training runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: NeuralNetMXNet_BAG_L0 ... Training model for up to 409.49s of the 409.49s of remaining time.\n",
            "\t0.0\t = Validation f1 score\n",
            "\t10.4s\t = Training runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: LinearModel_BAG_L0 ... Training model for up to 398.97s of the 398.97s of remaining time.\n",
            "\t0.0\t = Validation f1 score\n",
            "\t3.22s\t = Training runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Repeating k-fold bagging: 2/20\n",
            "Fitting model: RandomForest_BAG_L0 ... Training model for up to 395.66s of the 395.66s of remaining time.\n",
            "\t0.0\t = Validation f1 score\n",
            "\t10.6s\t = Training runtime\n",
            "\t1.02s\t = Validation runtime\n",
            "Fitting model: ExtraTrees_BAG_L0 ... Training model for up to 389.81s of the 389.81s of remaining time.\n",
            "\t0.0\t = Validation f1 score\n",
            "\t7.05s\t = Training runtime\n",
            "\t1.02s\t = Validation runtime\n",
            "Fitting model: KNeighbors_BAG_L0 ... Training model for up to 385.8s of the 385.8s of remaining time.\n",
            "\t0.0\t = Validation f1 score\n",
            "\t0.31s\t = Training runtime\n",
            "\t1.02s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L0 ... Training model for up to 385.09s of the 385.09s of remaining time.\n",
            "\t0.0\t = Validation f1 score\n",
            "\t13.32s\t = Training runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L0 ... Training model for up to 378.31s of the 378.31s of remaining time.\n",
            "\t0.0\t = Validation f1 score\n",
            "\t342.18s\t = Training runtime\n",
            "\t0.12s\t = Validation runtime\n",
            "Fitting model: NeuralNetMXNet_BAG_L0 ... Training model for up to 208.12s of the 208.11s of remaining time.\n",
            "\t0.0\t = Validation f1 score\n",
            "\t20.63s\t = Training runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: LinearModel_BAG_L0 ... Training model for up to 197.75s of the 197.75s of remaining time.\n",
            "\t0.0\t = Validation f1 score\n",
            "\t5.17s\t = Training runtime\n",
            "\t0.08s\t = Validation runtime\n",
            "Completed 2/20 k-fold bagging repeats ...\n",
            "Fitting model: WeightedEnsemble_L1 ... Training model for up to 360.0s of the 195.71s of remaining time.\n",
            "\t0.0\t = Validation f1 score\n",
            "\t0.66s\t = Training runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 404.98s ...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training process completed in 406.54304122924805s.\n",
            "Strating prediction process...\n",
            "Prediction process completed in 0.42626523971557617s.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>use_case</th>\n",
              "      <th>data_dir</th>\n",
              "      <th>label_col</th>\n",
              "      <th>sample_size</th>\n",
              "      <th>embedding_model</th>\n",
              "      <th>tokenizer_name</th>\n",
              "      <th>combiner_method</th>\n",
              "      <th>automl_model</th>\n",
              "      <th>automl_train_time</th>\n",
              "      <th>train_time</th>\n",
              "      <th>test_time</th>\n",
              "      <th>score</th>\n",
              "      <th>train_tokenization</th>\n",
              "      <th>train_embedding</th>\n",
              "      <th>train_combining</th>\n",
              "      <th>test_tokenization</th>\n",
              "      <th>test_embedding</th>\n",
              "      <th>test_combining</th>\n",
              "      <th>train_nrows</th>\n",
              "      <th>train_ncols</th>\n",
              "      <th>train_match_ratio</th>\n",
              "      <th>valid_nrows</th>\n",
              "      <th>valid_ncols</th>\n",
              "      <th>valid_match_ratio</th>\n",
              "      <th>test_nrows</th>\n",
              "      <th>test_ncols</th>\n",
              "      <th>test_match_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Structured/Beer</td>\n",
              "      <td>data/Structured/Beer/original/</td>\n",
              "      <td>label</td>\n",
              "      <td>None</td>\n",
              "      <td>bert</td>\n",
              "      <td>pair-sent-attr</td>\n",
              "      <td>avg</td>\n",
              "      <td>autogluon</td>\n",
              "      <td>600</td>\n",
              "      <td>406.543041</td>\n",
              "      <td>0.426265</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.924592</td>\n",
              "      <td>13.237058</td>\n",
              "      <td>0.017738</td>\n",
              "      <td>0.375757</td>\n",
              "      <td>4.880841</td>\n",
              "      <td>0.011133</td>\n",
              "      <td>268</td>\n",
              "      <td>4</td>\n",
              "      <td>14.925373</td>\n",
              "      <td>91</td>\n",
              "      <td>4</td>\n",
              "      <td>15.384615</td>\n",
              "      <td>91</td>\n",
              "      <td>4</td>\n",
              "      <td>15.384615</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          use_case  ... test_match_ratio\n",
              "0  Structured/Beer  ...        15.384615\n",
              "\n",
              "[1 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIvKv3HqGNp1"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}